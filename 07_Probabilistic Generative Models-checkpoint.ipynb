{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQs for Regression, MAP and MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So far we have focused on regression.  We began with the polynomial regression example where we have training data $\\mathbf{X}$ and associated training labels $\\mathbf{t}$ and we use these to estimate weights, $\\mathbf{w}$ to fit a polynomial curve through the data: \n",
    "\\begin{equation}\n",
    "y(x, \\mathbf{w}) = \\sum_{j=0}^M w_j x^j\n",
    "\\end{equation}\n",
    "\n",
    "* We derived how to estimate the weights using both maximum likelihood estimation (MLE) and maximum a-posteriori estimation (MAP). \n",
    "\n",
    "* Then, we said that we can generalize this further using basis functions (instead of only raising x to the jth power): \n",
    "\\begin{equation}\n",
    "y(x, \\mathbf{w}) = \\sum_{j=0}^M w_j \\phi_j(x)\n",
    "\\end{equation}\n",
    "where $\\phi_j(\\cdot)$ is any basis function you choose to use on the data. \n",
    "\n",
    "\n",
    "* *Why is regression useful?*  \n",
    "    * Regression is a common type of machine learning problem where we want to map inputs to a value (instead of a class label).  For example, the example we used in our first class was mapping silhouettes of individuals to their age. So regression is an important technique whenever you want to map from a data set to another value of interest. *Can you think of other examples of regression problems?*\n",
    "    \n",
    "    \n",
    "* *Why would I want to use other basis functions?*\n",
    "    * So, we began with the polynomial curve fitting example just so we can have a concrete example to work through but polynomial curve fitting is not the best approach for every problem.  You can think of the basis functions as methods to extract useful features from your data. For example, if it is more useful to compute distances between data points (instead of raising each data point to various powers), then you should do that instead!  \n",
    "    \n",
    "    \n",
    "* *Why did we go through all the math derivations? You could've just provided the MLE and MAP solution to us since that is all we need in practice to code this up.* \n",
    "    * In practice, you may have unique requirements for a particular problem and will need to decide upon and set up a different data likelihood and prior for a problem.  For example, we assumed Gaussian noise for our regression example with a Gaussian zero-mean prior on the weights.  You may have an application in which you know the noise is Gamma distributed and have other requirements for the weights that you want to incorporate into the prior.  Knowing the process used to derive the estimate for weights in this case is a helpful guide for deriving your solution.  (Also, on a practical note for the course, stepping through the math served as a quick review of various linear algebra, calculus and statistics topics that will be useful throughout the course.) \n",
    "    \n",
    "    \n",
    "* *What is overfitting and why is it bad?* \n",
    "    * The goal of a supervised machine learning algorithm is to be able to learn a mapping from inputs to desired outputs from training data.  When you overfit, you memorize your training data such that you can recreate the samples perfectly. This often comes about when you have a model that is more complex than your underlying true model and/or you do not have the data to support such a complex model.  However, you do this at the cost of generalization.  When you overfit, you do very well on training data but poorly on test (or unseen) data.  So, to have useful trained machine learning model, you need to avoid overfitting.  You can avoid overfitting through a number of ways.  The methods we discussed in class are using *enough* data and regularization.  Overfitting is related to the \"bias-variance trade-off\" (discussed in section 3.2 of the reading).  There is a trade-off between bias and variance. Complex models have low bias and high variance (which is another way of saying, they fit the training data very well but may oscillate widely between training data points) where as rigid (not-complex-enough) models have high bias and low variance (they do not oscillate widely but may not fit the training data very well either).  \n",
    "    \n",
    "    \n",
    "* *What is the goal of MLE and MAP?*\n",
    "    * MLE and MAP are general approaches for estimating parameter values.  For example, you may have data from some unknown distribution that you would like to model as best you can with a Gaussian distribution.  You can use MLE or MAP to estimate the Gaussian parameters to fit the data and determine your estimate at what the true (but unknown) distribution is.  \n",
    "    \n",
    "    \n",
    "* *Why would you use MAP over MLE (or vice versa)?* \n",
    "    * As we saw in class, MAP is a method to add in other terms to trade off against the data likelihood during optimization. It is a mechanism to incorporate our \"prior belief\" about the parameters.  In our example in class, we used the MAP solution for the weights in regression to help prevent overfitting by imposing the assumptions that the weights should be small in magnitude.  When you have enough data, the MAP and the MLE solution converge to the same solution.  The amount of data you need for this to occur varies based on how strongly you impose the prior (which is done using the variance of the prior distribution).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Generative Models\n",
    "\n",
    "* So far we have focused on regression.  We will begin to discuss classification. \n",
    "* Suppose we have training data from two classes, $C_1$ and $C_2$, and we would like to train a classifier to assign a label to incoming test points whether they belong to class 1 or 2.  \n",
    "* There are *many* classifiers in the machine learning literature.  We will cover a few in this class.  Today we will focus on probabilistic generative approaches for classification. \n",
    "* A *generative* approach for classification is one in which we estimate the parameters for distributions that generate the data for each class.  Then, when we have a test point, we can compute the posterior probability of that point belonging to each class and assign the point to the class with the highest posterior probability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%matplotlib inline  \n",
    "\n",
    "mean1 = [-1.5, -1]\n",
    "mean2 = [1, 1]\n",
    "cov1 = [[1,0], [0,2]]\n",
    "cov2 = [[2,.1],[.1,.2]]\n",
    "N1 = 250\n",
    "N2 = 100\n",
    "\n",
    "def generateData(mean1, mean2, cov1, cov2, N1=100, N2=100):\n",
    "    # We are generating data from two Gaussians to represent two classes. \n",
    "    # In practice, we would not do this - we would just have data from the problem we are trying to solve. \n",
    "    class1X = np.random.multivariate_normal(mean1, cov1, N1)\n",
    "    class2X = np.random.multivariate_normal(mean2, cov2, N2)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(*[1,1,1])\n",
    "    ax.scatter(class1X[:,0], class1X[:,1], c='r') \n",
    "    ax.scatter(class2X[:,0], class2X[:,1]) \n",
    "    plt.show()\n",
    "    return class1X, class2X\n",
    "    \n",
    "class1X, class2X = generateData(mean1, mean2,cov1,cov2, N1,N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the data we generated above, we have a \"red\" class and a \"blue\" class.  When we are given a test sample, we will want to assign the label of either red or blue.  \n",
    "\n",
    "We can compute the posterior probability for class $C_1$ as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(C_1 | x) &=& \\frac{p(x|C_1)p(C_1)}{p(x)}\\\\\n",
    "           &=& \\frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1) + p(x|C_2)p(C_2)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can similarly compute the posterior probability for class $C_2$: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(C_2 | x) &=& \\frac{p(x|C_2)p(C_2)}{p(x|C_1)p(C_1) + p(x|C_2)p(C_2)}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that $p(C_1|x) + p(C_2|x) = 1$. \n",
    "\n",
    "So, to train the classifier, what we need is to determine the parametric forms and estimate the parameters for $p(x|C_1)$, $p(x|C_2)$, $p(C_1)$ and $p(C_2)$. \n",
    "\n",
    "For example, we can assume that the data from both $C_1$ and $C_2$ are distributed according to Gaussian distributions.  In this case,\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{x}|C_k) = \\frac{1}{(2\\pi)^{1/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp\\left\\{ - \\frac{1}{2} (\\mathbf{x}-\\mu_k)^T\\Sigma_k^{-1}(\\mathbf{x}-\\mu_k)\\right\\}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Given the assumption of the Gaussian form, how would you estimate the parameter for $p(x|C_1)$ and $p(x|C_2)$?  *You can use maximum likelihood estimate for the mean and covariance!* \n",
    "\n",
    "The MLE estimate for the mean of class $C_k$ is: \n",
    "\\begin{eqnarray}\n",
    "\\mu_{k,MLE} = \\frac{1}{N_k} \\sum_{n \\in C_k} \\mathbf{x}_n\n",
    "\\end{eqnarray}\n",
    "where $N_k$ is the number of training data points that belong to class $C_k$\n",
    "\n",
    "The MLE estimate for the covariance of class $C_k$ is: \n",
    "\\begin{eqnarray}\n",
    "\\Sigma_{k,MLE} = \\frac{1}{N_k} \\sum_{n \\in C_k} (\\mathbf{x}_n - \\mu_{k,MLE})(\\mathbf{x}_n - \\mu_{k,MLE})^T\n",
    "\\end{eqnarray}\n",
    "\n",
    "We can determine the values for $p(C_1)$ and $p(C_2)$ from the number of data points in each class:\n",
    "\\begin{eqnarray}\n",
    "p(C_k) = \\frac{N_k}{N}\n",
    "\\end{eqnarray}\n",
    "where $N$ is the total number of data points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.4463101  -1.05228092]\n",
      "[[ 1.02951348  0.03276955]\n",
      " [ 0.03276955  2.03127459]]\n",
      "[ 1.06117615  0.94563646]\n",
      "[[ 2.38426029  0.09798675]\n",
      " [ 0.09798675  0.15495013]]\n",
      "0.7142857142857143\n",
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "#Estimate the mean and covariance for each class from the training data\n",
    "mu1 = np.mean(class1X, axis=0)\n",
    "print(mu1)\n",
    "\n",
    "cov1 = np.cov(class1X.T)\n",
    "print(cov1)\n",
    "\n",
    "mu2 = np.mean(class2X, axis=0)\n",
    "print(mu2)\n",
    "\n",
    "cov2 = np.cov(class2X.T)\n",
    "print(cov2)\n",
    "\n",
    "# Estimate the prior for each class\n",
    "pC1 = class1X.shape[0]/(class1X.shape[0] + class2X.shape[0])\n",
    "print(pC1)\n",
    "\n",
    "pC2 = class2X.shape[0]/(class1X.shape[0] + class2X.shape[0])\n",
    "print(pC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now have all parameters needed and can compute values for test samples\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "x = np.linspace(-5, 4, 100)\n",
    "y = np.linspace(-6, 6, 100)\n",
    "xm,ym = np.meshgrid(x, y)\n",
    "X = np.dstack([xm,ym])\n",
    "\n",
    "#look at the pdf for class 1\n",
    "y1 = multivariate_normal.pdf(X, mean=mu1, cov=cov1)\n",
    "plt.imshow(y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the pdf for class 2\n",
    "y2 = multivariate_normal.pdf(X, mean=mu2, cov=cov2);\n",
    "plt.imshow(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the posterior for class 1\n",
    "pos1 = (y1*pC1)/(y1*pC1 + y2*pC2 );\n",
    "plt.imshow(pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the posterior for class 2\n",
    "pos2 = (y2*pC2)/(y1*pC1 + y2*pC2 );\n",
    "plt.imshow(pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the decision boundary\n",
    "plt.imshow(pos1>pos2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How did we come up with using the MLE solution for the mean and variance? How did we determine how to compute $p(C_1)$ and $p(C_2)$?\n",
    "\n",
    "* We can define a likelihood for this problem and maximize it!\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{t}, \\mathbf{X}|\\pi, \\mu_1, \\mu_2, \\Sigma_1, \\Sigma_2) = \\prod_{n=1}^N \\left[\\pi N(x_n|\\mu_1, \\Sigma_1)\\right]^{t_n}\\left[(1-\\pi)N(x_n|\\mu_2, \\Sigma_2) \\right]^{1-t_n}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* *How would we maximize this?* As usual, we would use our \"trick\" and take the log of the likelihood function.  Then, we would take the derivative with respect to each parameter we are interested in, set the derivative to zero, and solve for the parameter of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
